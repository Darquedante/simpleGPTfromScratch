# Transformer implementation from scratch
A codebase implementing a simple GPT-like model from scratch based on the [Attention is All You Need paper](https://arxiv.org/abs/1706.03762).

## Getting Started 
Follow [setup instructions here](requirements.txt) to get started.
```
$ git clone https://github.com/bashnick/transformer.git
$ cd transformer
$ conda create --name transformer python=3.9 -y
$ conda activate transformer
$ pip install requirements.txt
```

## Contributing
You are welcome to contribute to the repository with your PRs!

## License

The MIT License (MIT)

Copyright (c) 2023 Nikolay Bashlykov
